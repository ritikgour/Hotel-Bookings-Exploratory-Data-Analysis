{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritikgour/Hotel-Bookings-Exploratory-Data-Analysis/blob/main/Bike_sharing_Demand_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### **Project Name**    - **Seoul Bike sharing Demand Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -**Seoul Bike sharing Demand Prediction**\n",
        "##### **Team Member 1 -Ritik Gour**\n",
        "##### **Team Member 2 -Niteesh Kumar**\n",
        "##### **Team Member 3 -Manish Kumar Prasad**\n",
        "##### **Team Member 4 -Ashish Kumar**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, many cities have introduced cycling to improve travel. It is important that rental cars are made on time and made available  to the public, as this reduces waiting time. Finally, maintaining a steady supply of rental bikes to the city is a major concern. An important part is estimating the number of bikes needed per hour to maintain a stationary bike supply.**"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do natural and human factors affect bike rental in Washington DC's urban bike share system?   Natural factors are season, month, day of the week, peak hours, working and non-working days, temperature, humidity, etc. Human factors characteristics include the location of the bike and regional characteristics.   I did a search for data analysis of these cases to respond to comments on the issue.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Modules \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting a drive and imporitng the data base**"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qMa6caZYc2xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data set from the drive \n",
        "Bike_df=pd.read_csv('/content/drive/MyDrive/Bike sharing/SeoulBikeData.csv',encoding ='latin')"
      ],
      "metadata": {
        "id": "PWDBF6VQc7ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "Bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.tail()"
      ],
      "metadata": {
        "id": "W04WpRWRe_mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "Bike_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "Bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Their is 0 duplicate value."
      ],
      "metadata": {
        "id": "wvxnQ4wzgzyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "Bike_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "Bike_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   The dataset contain 8760 row and 14 columns.\n",
        "*   In 24 hours a day we have 365 days in a year we multiplied 365*24=8760. It's represents number of line in the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of Features.\n",
        "### The columns and the data it represents are listed below:"
      ],
      "metadata": {
        "id": "Al-JlswkqlND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown of our features**  \n",
        "\n",
        "**Date : The date of the day, during a 365 days from 01/12/2017 to 30/11/2018, fromating in DD/MM/YYYY, type:str, we need to convert into date time format.**\n",
        "\n",
        "**Rented bike count : No of rented bikes per hour which our dependent variable and we need to predicat that, type:int,**\n",
        "\n",
        "**Hour: The hour of the day, starting from 0-23 it's in a digital time formate, type:int, we need to convert it into category data type.**\n",
        "\n",
        "\n",
        "**Temperature(°C) : Temperature in celsius, type:float**\n",
        "\n",
        "**Humidity(%) : Humidity in the air in %, type:float**\n",
        "\n",
        "**Wind speed (m/s) : Speed of the wind in m/s, type:float**\n",
        "\n",
        "**Visibility (10m) : Visibility In m, type:int** \n",
        "\n",
        "**Dew point temperature(°C) : Temperature at the begning of the day, type:float**\n",
        "\n",
        "**Solar Radiation (MJ/m2) : Sun contribution, type:float**\n",
        "\n",
        "**Rainfall(mm) : Ammount of ranining in mm, type:float**\n",
        "\n",
        "**Snowfall (cm) : Ammount of Snowing in cm, type:float**\n",
        "\n",
        "**Seasons : Season of the year, type:str, there are only four seasons in data.**\n",
        "\n",
        "**Holiday : If the day is holiday period or not, type:str**\n",
        "\n",
        "**Functioning Day : If the day is functioning day or not, type:str**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4YZvn2lq67r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Check Unique Values for each variable.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ESBeMMwPz12h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.nunique()"
      ],
      "metadata": {
        "id": "t1bwfl7Sz-SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***Processing the dataset***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why we need to handle missing values?**\n",
        "\n"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**World data is often multidimensional. \n",
        "Missing results may be due to incorrect or missing data. Processing missing data during data preprocessing is crucial because many machine learning algorithms do not support missing values. So first we check for missing values**  "
      ],
      "metadata": {
        "id": "l_ZaetFIdt2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "LHYIipX6eh-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "Bike_df.isna().sum()\n",
        "Bike_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "_L9EJhOFejy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Missing Values/Null Values Using heatmap\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.heatmap(Bike_df.isnull(),cmap='viridis',annot=False,yticklabels=False)\n",
        "plt.title(\" Visualizing a Missing Values\")"
      ],
      "metadata": {
        "id": "gupxsxoDesER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can their is no missing or Null value.**"
      ],
      "metadata": {
        "id": "kYP0KN-8e-3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "DXFORauzfL2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**That's why it's important to remove duplicate records from data.**"
      ],
      "metadata": {
        "id": "DJTZGWFFfdri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Duplicate\" simply means you have a duplicate file in your file. This may be due, for example, to incorrect data entry or data collection procedures. By eliminating duplicates in our database, we don't send multiple communications to the same person, saving time and money."
      ],
      "metadata": {
        "id": "j68Jgtjsf7bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cheking duplicate values**"
      ],
      "metadata": {
        "id": "_3DoUq-DgFoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value=len(Bike_df[Bike_df.duplicated()])\n",
        "print(\"no of duplicate values in the datadet is =\",value)"
      ],
      "metadata": {
        "id": "UOzcyg_jgQDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The above data after count the missing and duplicate value we saw there are no missing values are present.**\n",
        "\n",
        "**Some of the columns name the dataset are too large and clumsy so we can change some simple names, and it don't affect our final results.** \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GHWNEXgDg92p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing Column name."
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the complex columns name\n",
        "Bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count', \n",
        "                        'Temperature(°C)':'Temperature', \n",
        "                        'Humidity(%)':'Humidity',\n",
        "                        'Wind speed (m/s)':'Wind speed', \n",
        "                        'Visibility (10m)':'Visibility', \n",
        "                        'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                        'Solar Radiation (MJ/m2)':'Solar_Radiation', \n",
        "                        'Rainfall(mm)':'Rainfall','Snowfall (cm)':'Snowfall',\n",
        "                        'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "mQfE8gHHiDjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python reads column \"date\" as an object type, basically  as a string, because date column is very important for analyzing user's behavior, so we need to convert it to date type text and split it into 3 columns, for example 'year', ' month', 'day' are categorical data types.**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Breaking Date Column**"
      ],
      "metadata": {
        "id": "962OnTTSlr7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing \"Date\" column into three \"Year\",\"Month\",\"Day\" column. \n",
        "Bike_df['Date']=Bike_df['Date'].apply(lambda x:dt.datetime.strptime(x,\"%d/%m/%Y\"))\n"
      ],
      "metadata": {
        "id": "DIVhV0m-l_jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df['Year']=Bike_df['Date'].dt.year\n",
        "Bike_df['Month']=Bike_df['Date'].dt.month\n",
        "Bike_df['Day']=Bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "GTGQhjKKnxGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new column of \"weekends_weekend\" and drop the column \"Date\",\"Day\",\"Year\".\n",
        "Bike_df['weekends_weekend'] = Bike_df['Day'].apply(lambda x: 1 if x == 'Saturday' or x == 'Sunday' else 0)\n",
        "Bike_df = Bike_df.drop(columns=['Date','Day','Year'],axis=1)\n"
      ],
      "metadata": {
        "id": "tXZO0JErolxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Converting date column into 3 different column i.e \"Year\", \"Month\",\"Day\"\n",
        "*   The \"Year\" column in our data intially contains 2 unique numbers with contains 2017 December to 2018 November, so i think it's a year we don't need \"Year\" row, so we are removing or drop it.\n",
        "*   Another 'day' column with details about each day of the month, we don't need data for each day of the month for our relationship, but if a day is a working day or a holiday, we need data about what we're converting. set it to this format and display the \"day\" line.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M5NibNmkqCp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.head()"
      ],
      "metadata": {
        "id": "ZdhcUmUmsLhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.info()"
      ],
      "metadata": {
        "id": "KB950ZvssSF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df['weekends_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "OU_GjJBdsakW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing Data Types"
      ],
      "metadata": {
        "id": "dtOePgzgsv7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Columns such as \"Hour\", \"Month\", \"Weekday Weekend\" are displayed as equal data, but are actually a categorical data type. So we need to change this information and if we don't, when we do further analysis and interact with it, the values ​​are not correct so we can make mistakes.**"
      ],
      "metadata": {
        "id": "bA2DuysetdxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.nunique()"
      ],
      "metadata": {
        "id": "deDCji6RtjHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tables.table import Cols\n",
        "# Change the int64 into category column\n",
        "cols=['Hour','Month','weekends_weekend']\n",
        "for col in cols:\n",
        "  Bike_df[col]= Bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "6thx89Mktwdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the result data type\n",
        "Bike_df.info()"
      ],
      "metadata": {
        "id": "fcwwgIJ3vDr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.columns"
      ],
      "metadata": {
        "id": "sHUT4a_svMAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df['weekends_weekend'].unique()"
      ],
      "metadata": {
        "id": "T_pfz926vaNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###***Exploratory Data Analysis Of Data set.***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Why do we perfrome EDA?**"
      ],
      "metadata": {
        "id": "JbWry55NZDn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA is a Exercising analysis designed to reveal the underlying structure of data and is important to companies as it reveals subtle patterns, patterns and relationships.**"
      ],
      "metadata": {
        "id": "8n1CZBF2ZM-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Univariate Analysis**"
      ],
      "metadata": {
        "id": "pbznl4lHZlFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Why we do univariate analysis?**"
      ],
      "metadata": {
        "id": "I2D2-4zMaB1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  *   **The main purpose of univariate analysis is to simply describe the data to find patterns in the data.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cNaLviciaX8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Analysis Of Dependent Varible:**"
      ],
      "metadata": {
        "id": "84eG9tbGalWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**What is dependent variable in data anlysis?**"
      ],
      "metadata": {
        "id": "jvfRskZaa5nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We define three variables, which are variables whose value changes depending on the value of another variable.**"
      ],
      "metadata": {
        "id": "RF0G5-wUbMym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Analaysation Of Categorical Variables :**"
      ],
      "metadata": {
        "id": "kkVsCb09bZ4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our variable is \"Number of Bike Rentals\" so we need to compare this column with other columns using a view. First we determine the categorical data types, then we move on to the phone data**"
      ],
      "metadata": {
        "id": "1GZ9MWnobudm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of data by Visualization.\n"
      ],
      "metadata": {
        "id": "ry_ZqpOWcALv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating numerical columns list\n",
        "numeric_col=Bike_df.describe().columns.tolist()\n",
        "numeric_col\n",
        "\n"
      ],
      "metadata": {
        "id": "L0qDEKFshg3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_col:\n",
        "    fig, ax = plt.subplots(figsize=(18, 5))\n",
        "    sns.lineplot(x=Bike_df.index, y=Bike_df[col], ax=ax)\n",
        "    ax.set_xlabel('index')\n",
        "    ax.set_ylabel(col)\n",
        "    ax.set_title(f'Line plot of {col}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "80MZIu8QnRIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating categorical columns list\n",
        "categorical_columns=list(set(Bike_df.columns) - set(Bike_df.describe().columns))\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "g_SJOD9mjYi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_columns:\n",
        "  fig, ax = plt.subplots(figsize=(18,5))\n",
        "  sns.boxplot(x=col, y='Rented Bike Count', data=Bike_df, ax=ax)\n",
        "  ax.set_xlabel(col)\n",
        "  ax.set_ylabel('Rented Bike Count')\n",
        "  ax.set_title(f'Boxplot of {col} with respect to Rented Bike Count')\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QUsrMa5_uaej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Observation From Above Charts.**"
      ],
      "metadata": {
        "id": "38fynhk3xZw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Cycling is at its highest in summer. Most people will rent bikes during the summer months. Bicycle prices in winter are quite low compared to other seasons.\n",
        "\n",
        "*   Many bikes are rented \"when there is no vacation.\n",
        "*   There are no bicycles for rent on non-working days. We only have 295 \"no\" numbers. So we can put 295 values ​​but it just won't add value to \"yes\" column. So the lines don't end for us. We will delete them in the next step.\n",
        "\n",
        "\n",
        "*   Most bikes are rented on weekdays, not weekends.\n",
        "\n",
        " \n",
        "*   The number of bicycles starts to increase from March and peaks in June. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "raOUbSctxnkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###   **Let's Check bike rent count trend with respect to Hour on Functioning days weekend or weekdays, Seasons, Holidays columns.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating categorical columns list\n",
        "categorical_columns=list(set(Bike_df.columns) - set(Bike_df.describe().columns))\n",
        "categorical_columns\n"
      ],
      "metadata": {
        "id": "pj7RVW942_8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bike Rental Trend With Respect To Hour on Holiday or No Holiday.\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.barplot(x = Bike_df['Hour'], y=Bike_df['Rented Bike Count'], hue=Bike_df['Holiday'])\n",
        "plt.title(\"Bike Rental Trend According To Hour On Holiday / No Holiday\")\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "TDVKLNqkz48N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Sudden peak between 6/7 and 10:00. Working hours, college and office hours may be responsible for this sudden happening. (There is no holiday). But on holidays, things are different and there are fewer bikes.\n",
        "\n",
        "*   There is another peak between 10:00 in the morning and 19:00 in the evening. It may be time to work on the people above. (There is no holiday).\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bike Rental Trend With Respect To Hour on Funtioning day.\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.barplot(x = Bike_df['Hour'], y=Bike_df['Rented Bike Count'], hue=Bike_df['Functioning Day'])\n",
        "plt.title(\"Bike Rental Trend According To Hour On Functioning Day\")"
      ],
      "metadata": {
        "id": "nNd73aHczO_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) The formula here is the same for non-working days. The only difference is that there is no cycling on non-work days.**\n",
        "\n",
        "**2) If we do not remove the line on the working day. We only collect business day prices. So a column with a default value doesn't always help.**"
      ],
      "metadata": {
        "id": "F9vNbpED0gdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bike Rental Trend With Respect To Hour on Seasons.\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.barplot(x = Bike_df['Hour'], y=Bike_df['Rented Bike Count'], hue=Bike_df['Seasons'])\n",
        "plt.title(\"Bike Rental Trend According To Hour On Seasons\")"
      ],
      "metadata": {
        "id": "tCRhwG1Q08xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bike Rental Trend With Respect To Hour on Month.\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.barplot(x = Bike_df['Hour'], y=Bike_df['Rented Bike Count'], hue=Bike_df['Month'])\n",
        "plt.title(\"Bike Rental Trend According To Hour On Month\")"
      ],
      "metadata": {
        "id": "K9TN3QQL1jY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bike Rental Trend With Respect To Hour on weekends_weekend .\n",
        "plt.figure(figsize=(19,5))\n",
        "sns.barplot(x = Bike_df['Hour'], y=Bike_df['Rented Bike Count'], hue=Bike_df['weekends_weekend'])\n",
        "plt.title(\"Bike Rental Trend According To Hour On weekends_weekend\")"
      ],
      "metadata": {
        "id": "mCfmyPu92B2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Analyzing of Numerical Variables.**"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is numerical data**"
      ],
      "metadata": {
        "id": "tI0eEIXOoBw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Numeric data is a type of data that is represented by numbers rather than descriptors. Numerical data, sometimes called quantitative data, is always written in numbers. Numeric data is distinguished from other data types by the ability to perform arithmetic operations on these numbers.**"
      ],
      "metadata": {
        "id": "R10PQZm9odUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign a numerical column to variable\n",
        "numerical_columns = list(Bike_df.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features = pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "y65mJ96WpDB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing displot to analying the distribution of all numerical features\n",
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(18,5))\n",
        "  sns.distplot(x=Bike_df[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tsQn-x44qHmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observation :**\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "*   **From the picture above, we can see that people like to ride a bike in hot weather with an average temperature of 25°C.**\n",
        "\n",
        "*   **It's almost the same as \"temperature\" from the \"dew_point_temperature\" figure above, we can check it in the next step.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   **From the picture above we can see that the bike rental is huge and in the case of solar power the counter of the rental price is around 1000**\n",
        "\n",
        "*   **From the graph, we can see that the number of rental cars on the y-axis is very small. Cycling stops when more than 4 cm of snow falls.**\n",
        "\n",
        "\n",
        "\n",
        "*   **From the above figure, we see that even if it rains a lot, the demand for rental bikes does not decrease, for example, even if it rains for 20 months, the demand for cars is still high.**\n",
        "\n",
        "*   **We understand from the figure above that the demand for bike rental is the same regardless of the wind speed, but the demand for bikes increases when the wind speed is 7 m/s Plus, people like to rent bikes. Cycling when the wind is low.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v9DBDEOl45jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression**"
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **Regression charts in Seaborn are often used during data analysis to add visual direction to help highlight patterns in datasets. As the name suggests, a standard chart creates a regression line between two parameters and helps to visualize the relationship between them.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing  regression plot for all the numerical features.\n",
        "for col in numerical_features:\n",
        "  fig,ax=plt.subplots(figsize=(18,5))\n",
        "  sns.regplot(x = Bike_df[col], y =  Bike_df['Rented Bike Count'], scatter_kws={\"color\":'yellow'}, line_kws={\"color\":\"black\"} )"
      ],
      "metadata": {
        "id": "mT03PcgPmA-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  **From the regression graphs for all the numbers above, we see that the columns 'Temperature', 'Wind_speed', 'Visibility', 'Dew_point_temperature', 'Solar_Radiation' have a positive effect with the target variable.**\n",
        "\n",
        "*   **This means that rental bikes are increasingly equipped with these features.**\n",
        "\n",
        "\n",
        "\n",
        "*   **\"Rain\", \"snowfall\", \"humidity\" attributes are negatively related to the target variable, that is, when these attributes increase, bike rental decreases.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2H0FLzwjn-12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalise Ratned Bike Count Column Data**"
      ],
      "metadata": {
        "id": "xuIPTql3oxnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data normalization (also known as data prioritization) is an important part of data mining. This means modifying the data, i.e. converting the data into another format that allows it to be processed efficiently. The main purpose of data normalization is to reduce or even eliminate duplicate data.**"
      ],
      "metadata": {
        "id": "iUQTPvNQpOKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution plot of Ranted Bike Count\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.xlabel('Ranted Bike Count')\n",
        "plt.ylabel('Density')\n",
        "ax=sns.histplot(Bike_df['Rented Bike Count'], color=\"y\")\n",
        "ax.axvline(Bike_df['Rented Bike Count'].mean(), color='blue', linestyle='dashed',linewidth=2)\n",
        "ax.axvline(Bike_df['Rented Bike Count'].median(), color='green', linestyle='dashed',linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hQV52QdBqAW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The image above shows an error in the Rental Bike Count. Since the assumption of linear regression is that \"the distribution of the variable must be normal\", we need to do some work to normalize it.**"
      ],
      "metadata": {
        "id": "KWlvkngntv1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot of Ranted Bike Count To Check Outliers\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.ylabel('Rented Bike Count')\n",
        "sns.boxplot(x= Bike_df['Rented Bike Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZCr3k-O2t5mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The above boxplot shows that we have defect outliers in Rented Bike Count Column**"
      ],
      "metadata": {
        "id": "OAVanLMSvaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Square Root To Rented Bike Count to Improve Skewneww\n",
        "\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.histplot(np.sqrt(Bike_df['Rented Bike Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(Bike_df['Rented Bike Count']).mean(), color='blue', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(Bike_df['Rented Bike Count']).median(), color='green', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOiVmGZqvphU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **We have the general rule of using square root to keep skewed variables normal. Applying the square root to the number of skewed rental bikes, we get an almost normal distribution here.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2np0roi_x7fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.ylabel('Rented Bike Count')\n",
        "sns.boxplot(x=np.sqrt(Bike_df['Rented Bike Count']))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vPMIs7ksyEdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.corr()"
      ],
      "metadata": {
        "id": "WIoPdHbxyzMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After applying square root Rented Bike Count Column, we find that there is no outliers present.**"
      ],
      "metadata": {
        "id": "KUfp6blky57l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Checking corelation between dependent and independent variable.**"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,8))\n",
        "corr=Bike_df.corr()\n",
        "sns.heatmap(data=abs(corr), annot=True, cmap='cividis')"
      ],
      "metadata": {
        "id": "jsMOBjzkOdTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **We can obsserve on the heatmap that on the target variable line the most positively correlated variables to the rent are.**\n",
        "\n",
        "\n",
        "*   **The Temperature**\n",
        "*   **The Dew Point Temperature**\n",
        "\n",
        "### **And most negatively correlated variables are: And most negative correlated variables are:**\n",
        "\n",
        "*   **Rainfall**\n",
        "*   **Humidity**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzkZmYsAWKjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **From the correlation heatmap above, we can see that there is a good correlation of 0.91 between the \"Dark\" column and the \"Dew Point Temperature\" column, so even if we drop this column, it will not affect our survey results. They have the same variation.So we can drop the column \"Dew point temperature (°C)**"
      ],
      "metadata": {
        "id": "TRMBC2zWYJYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the Dew point temperature column\n",
        "Bike_df.drop('Dew point temperature(°C)',axis=1)"
      ],
      "metadata": {
        "id": "_n-S0Q0FYpLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df.info()"
      ],
      "metadata": {
        "id": "8nQB3UxTvCfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating dummy variables** "
      ],
      "metadata": {
        "id": "cLtdQIvQvVT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset can have different types of values ​​and sometimes have categorical values. Therefore, we create dummy variables to be able to work effectively with categorical values.**"
      ],
      "metadata": {
        "id": "yzg025Novs4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign all categorical features to a variabes\n",
        "categorical_features=list(Bike_df.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "oWIo_kWivyIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One hot encoding**"
      ],
      "metadata": {
        "id": "nZVKaPl0wjnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gold coding allows for multiple representations of categorical information. Many machine learning algorithms cannot directly process categorical data. The category must be converted to a number. This is required for both input and output variables for classification.**"
      ],
      "metadata": {
        "id": "a4B1aR7Kwqob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df_copy = Bike_df\n",
        "def one_hot_encoding(data,column):\n",
        "  data = pd.concat([data,pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "  data = data.drop([column],axis=1)\n",
        "  return data\n",
        "\n",
        "for col in categorical_features:\n",
        "  Bike_df_copy = one_hot_encoding(Bike_df_copy , col)\n",
        "Bike_df_copy.head()"
      ],
      "metadata": {
        "id": "kOE_OnNzw3_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training** "
      ],
      "metadata": {
        "id": "f8vThhINy6Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Train test split for regression**"
      ],
      "metadata": {
        "id": "luuUHJvMzHgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before fitting a model, divide the data empirically into training and testing. This means that some of the data will be used to train the model and some will be used to test our model's performance on unseen data. The ratio varies from person to person, for example 60:40, 70:30, 75:25, but the most common is 80:20 for training and testing, respectively. In this step, we will use the scikit-learn library to split the data into training and testing.**"
      ],
      "metadata": {
        "id": "YFv465HFzb06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the value in x and y\n",
        "x = Bike_df_copy.drop(columns=['Rented Bike Count'] , axis=1)\n",
        "y = np.sqrt(Bike_df_copy['Rented Bike Count'])\n",
        "\n"
      ],
      "metadata": {
        "id": "IWR_R5BHzk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.head()"
      ],
      "metadata": {
        "id": "wJ7M3miV0atx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "TGBW9WDi0m0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creat test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "wBKTPW950ud5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "MM-H-y2P1w2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * **The mean square error (MSE) tells you how close the regression line is to the points. It does this by calculating the distance between points from the regression line (these distances are \"errors\") and squaring them.\n",
        "It is called the squared error because you are looking for the mean of error states. The lower the MSE, the better the estimate.**\n",
        "\n",
        "* **MSE formula = (1/n) * Σ(actual - forecast)2 Where:**\n",
        "* **n = number of items,**\n",
        "* **Σ = summation notation,**\n",
        "* **Actual = original or observed y-value,**\n",
        "* **Forecast = y-value from regression.**\n",
        "\n",
        "* **The root mean square error (RMSE) is the standard deviation of the residuals (estimation error).**\n",
        "\n",
        "* **Mean Absolute Error (MAE) is a metric used to evaluate regression models. ...where the error is the difference between the predicted value (the value predicted by our regression model) and the actual value of the variable.**\n",
        "\n",
        "* **R-squared (R2) is a statistical test that shows the amount of variance in the variance explained by one or more variables in a regression model.**\n",
        "\n",
        "* **Formula for R-Squared**\n",
        "\\begin{aligned} &\\text{R}^2 = 1 - \\frac{ \\text{Unexplained Variation} }{ \\text{Total Variation} } \\\\ \\end{aligned} \n",
        "\n",
        "*   **R 2= 1 - Total Variation Unexplained**\n",
        "\n",
        "* **The adjusted R-square is a modified version of the R-squared adjusted for the number of predictors in the sample.**"
      ],
      "metadata": {
        "id": "V9i8c4zj2iIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LINEAR RESRESSION**"
      ],
      "metadata": {
        "id": "HTScFvYZ6Hlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A regression model explains the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line**"
      ],
      "metadata": {
        "id": "uNlDXK9G6eqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression uses the output method to model the relationship between independent variables and variables. In short, it is the most convenient line drawn between the values ​​of freedom and dependency. In the univariate case, the model is the same as the equation of a line with intersection and slope.**\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "**where** $$\\beta_0 \\text{ and } \\beta_1$$ **are intercept and slope respectively**.\n",
        "\n",
        "**In case of multiple features the formula translates into:**\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\beta_3x_3 +.....$$\n",
        "\n",
        "**where x_1,x_2,x_3 are the features values and **\n",
        "\n",
        "$$\\beta_0,\\beta_1,\\beta_2.....$$\n",
        "\n",
        " **are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.**\n",
        "\n",
        " **Gradient descent is a technique in which an algorithm attempts to improve automatic loss of function. Unemployment is nothing more than the difference between the actual value and the predicted value (also called error or residual). There are many types of unemployment, but this is the simplest. The sum of all observations gives the unemployment rate.**\n",
        "\n",
        "**The task of gradient descent is to update the parameters until the function value i decreases.That is, the global minimum has been reached. It uses the \"alpha\" hyperparameter to weight the value of the function and determine how long a step will take. Alpha is called the learning rate. It is always necessary to maintain a positive value of alpha because high and low alpha cause gradient drop or get stuck at a local minimum.\n",
        "Before using the algorithm, some basic assumptions need to be met. these**\n",
        "\n",
        "1. **No multicollinearity in the dataset.**\n",
        "\n",
        "2. **Independent variables should show linear relationship with dv.**\n",
        "\n",
        "3. **Residual mean should be 0 or close to 0.**\n",
        "\n",
        "4. **There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.**\n",
        "\n",
        "\n",
        "**Let us now implement our first model.\n",
        "We will be using LinearRegression from scikit library.**"
      ],
      "metadata": {
        "id": "B1FQc3FT7Av_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the package \n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression ().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8vQ3pFdR8ewD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the score \n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "5_UeA1zj89vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the Coefficient\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "gpu8MK0t9sbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the X_train and X-test value\n",
        "y_pred_train = reg.predict(X_train)\n",
        "y_pred_test = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "f1xXqB-k-BvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package \n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train),(y_pred_train))\n",
        "print(\"MSE:\" , MSE_lr)\n",
        "\n",
        "#Calculate RMSE\n",
        "RMSE_lr = np.sqrt(MSE_lr)\n",
        "print(\"RMSE:\" , RMSE_lr)\n",
        "\n",
        "#Calculate MAE\n",
        "MAE_lr = mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE:\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjussted r2\n",
        "r2_lr=r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr=(1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ))\n",
        "print(\"Adjusted R2:\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1))/((X_test.shape[1]-1) ))"
      ],
      "metadata": {
        "id": "oBOnvAtQ-c6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It looks like our r2 value is 0.77, which means our model is able to capture most of the differences in the data. Let's save this in the comparison file for later comparison.**"
      ],
      "metadata": {
        "id": "Vi-897a_CN9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "\n",
        "dict2={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((MSE_lr),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_lr),2)\n",
        "}\n",
        "\n",
        "test_df = pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "MhHaKBxFCbWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadcity \n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))"
      ],
      "metadata": {
        "id": "zGviqRNWDt0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the figure\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predict\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7HVayeZan0vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lasso Regression** "
      ],
      "metadata": {
        "id": "Wp0j_qI7p2LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an intances of lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 1.0, max_iter = 3000)\n",
        "\n",
        "# Fit the Lasso Model\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test),lasso.score(X_train, y_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "QzvVtf22qKxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the X_train and X-test value\n",
        "y_pred_train_lasso = lasso.predict(X_train)\n",
        "y_pred_test_lasso = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "ay2bCCp7r0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "MSE_l = mean_squared_error((y_train),(y_pred_train_lasso))\n",
        "print(\"MSE:\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l = np.sqrt(MSE_l)\n",
        "print(\"RMSE:\",RMSE_l)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_l = mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE:\",MAE_l)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_l = r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2:\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]/(X_test.shape[0]-X_test.shape[1]-1) )) )\n",
        "print(\"Adjusted R2:\",(1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]/(X_test.shape[0]-X_test.shape[1]-1)) ) ))\n",
        "\n"
      ],
      "metadata": {
        "id": "qFk2_071seCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It looks like our r2 score is 0.40, which means our model didn't capture most of the variance in the data. Let's save this in the comparison file for later comparison.**"
      ],
      "metadata": {
        "id": "mNWlvFKryAQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict1={'Model':'Lasso regression',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((MSE_l),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_lr),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "8PhFBX7cyWvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import  mean_squared_error\n",
        "\n",
        "# Clalculate MSE\n",
        "MSE_l = mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE:\",MSE_l)\n",
        "\n",
        "# Calculate RMSE\n",
        "RMSE_l = np.sqrt(MSE_l)\n",
        "print(\"RMSE:\",RMSE_l)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_l = mean_absolute_error(y_test,y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_l = r2_score((y_test),(y_pred_test_lasso))\n",
        "print(\"R2:\",r2_l)\n",
        "Adjusted_R2_l= (1-(1-r2_score((y_test),(y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ))\n",
        "print(\"Adjusted R2:\" ,1-(1-r2_score((y_test),(y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "MoU6ne5N0Pe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the r2_score of the test set is 0.38. This means that our linear model does not perform well on the data. Let's try to visualize our residuals to see if there is hetero variance (even or not dispersed).**"
      ],
      "metadata": {
        "id": "l6m9oNcE5eMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((MSE_lr),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_lr),2)\n",
        "}\n",
        "\n",
        "test_df = pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "poHt63RB5nU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the figure\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predict\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fiR1Ch7q5tq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadcity\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))"
      ],
      "metadata": {
        "id": "0l_wN8n858XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Ridge Regfression**"
      ],
      "metadata": {
        "id": "Qp3YEydx6cNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha = 0.1)"
      ],
      "metadata": {
        "id": "8z0FRFm96po9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the Model\n",
        "ridge.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "v2mhFvqT7C5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the Score\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "tYYRjo4E7WWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the X_train and X-test value\n",
        "y_pred_train_ridge = ridge.predict(X_train)\n",
        "y_pred_test_ridge = ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "h5OtGCq37lfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "MSE_r=mean_squared_error((y_train),(y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#Calculate RMSE\n",
        "RMSE_r = np.sqrt(MSE_r)\n",
        "print(\"RMSE:\",RMSE_r)\n",
        "\n",
        "#Calculate MAE \n",
        "MAE_r = mean_absolute_error (y_train, y_pred_train_ridge)\n",
        "print(\"MAE:\",MAE_r)\n",
        "\n",
        "#import package\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r=r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train,y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ))\n",
        "print(\"Adjusted R2:\",1-(1-r2_score(y_train,y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ))"
      ],
      "metadata": {
        "id": "8Y49BJ7P8lem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It seems that our r2 score is 0.77, which means our model was able to capture most of the data variables. Let's save this in the comparison file for later comparison.**"
      ],
      "metadata": {
        "id": "eLv-oBJ0JHND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((MSE_r),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_r),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "zsY_lHNhJQu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Calculate MSE\n",
        "MSE_r = mean_squared_error(y_test,y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#Calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "#Calculate MAE\n",
        "MAE_r=mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE:\",MAE_r)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_r = r2_score((y_test),(y_pred_test_ridge))\n",
        "print(\"R2:\",r2_r)\n",
        "Adjusted_R2_r = (1-(1-r2_score((y_test),(y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test),(y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )"
      ],
      "metadata": {
        "id": "wdKK-PcqJdDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the r2_score of the test set is 0.78. This means that our linear model performs well on the data. Let's try to visualize our residuals to see if there is hetero variance (even or not dispersed).**"
      ],
      "metadata": {
        "id": "1R7rP_z1NlMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((MSE_r),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_r),2)\n",
        "}\n",
        "\n",
        "test_df = pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "BHcvAM32NSoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the figure\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.plot(y_pred_test_ridge)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predict\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x565Om5MNp4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadcity\n",
        "plt.scatter((y_pred_test_ridge),(y_test-y_pred_test_ridge))"
      ],
      "metadata": {
        "id": "08YC5-JbOAdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Elastic Net Regression**"
      ],
      "metadata": {
        "id": "XKEg2wPy3Tvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package\n",
        "from sklearn.linear_model import ElasticNet\n",
        "#a* L1 + b* L2\n",
        "#alpha = a + b and l1_ratio = a/(a+b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "kwdLZr463g45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit The Model\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "eVUiqq5g4_2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "hD7HJBdm5QsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the X_train and X-test Value\n",
        "y_pred_train_en=elasticnet.predict(X_train) \n",
        "y_pred_test_en=elasticnet.predict(X_test)\n"
      ],
      "metadata": {
        "id": "uNRgyfx65kS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#calculate MSE\n",
        "MSE_e=mean_squared_error((y_train),(y_pred_train_en))\n",
        "print(\"MSE:\",MSE_e)\n",
        "\n",
        "#Calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "#Calculate MAE\n",
        "MAE_e=mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE:\",MAE_e)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_e = r2_score((y_train),(y_pred_train_en))\n",
        "print(\"R2:\",r2_e)\n",
        "Adjusted_R2_e = (1-(1-r2_score((y_test),(y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test),(y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "\n"
      ],
      "metadata": {
        "id": "4obCE1iT6ACJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It looks like our r2 score is 0.62, which means our model was able to capture most of the differences in the data. Let's save this in the comparison file for later comparison.**"
      ],
      "metadata": {
        "id": "sNJ9FpS78g1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((MSE_r),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_r),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "BEQmpj1V8sRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "#calculate MSE\n",
        "MSE_e=mean_squared_error((y_test),(y_pred_test_en))\n",
        "print(\"MSE:\",MSE_e)\n",
        "\n",
        "\n",
        "#Calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "#Calculate MAE\n",
        "MAE_e=mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE:\",MAE_e)\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "\n"
      ],
      "metadata": {
        "id": "6vBc0YP7QijE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the figure\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.plot(y_pred_test_en)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predict\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GUusUdw38vSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadcity\n",
        "plt.scatter((y_pred_test_en),(y_test-y_pred_test_en))"
      ],
      "metadata": {
        "id": "I_KkZ-mI9C8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree**"
      ],
      "metadata": {
        "id": "Fv5iHRNW9PrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import decision tree packages\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create an object\n",
        "D_tree=DecisionTreeRegressor(max_depth=9,splitter='best',max_features='auto')\n",
        "\n",
        "# fitting Xtrain ytrain\n",
        "D_tree.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "PpRuGhq09ZpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # check score on train dataset\n",
        "D_tree.score(X_train,y_train)"
      ],
      "metadata": {
        "id": "fSEIIPL_SaF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check score on the dataset.\n",
        "D_tree.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "-ltkPB2XSqTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  predicting y_train_predictions from  X train values\n",
        "y_pred_train=D_tree.predict(X_train)\n",
        "#  predicting y_predictions from  X test values\n",
        "y_pred=D_tree.predict(X_test)\n",
        "     "
      ],
      "metadata": {
        "id": "sXuyn7tMTJoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of ttraning data set\n",
        "\n",
        "# calculate mean absoluate error.\n",
        "MAE = mean_absolute_error(y_train,y_pred_train)\n",
        "print(MAE)\n",
        "\n",
        "# Calculate Mean square error\n",
        "MSE=mean_squared_error(y_train,y_pred_train)\n",
        "print(MSE)\n",
        "\n",
        "# Calculate root mean squared error\n",
        "RMSE=np.sqrt(MSE)\n",
        "print(RMSE)\n",
        "\n",
        "# calculate R2 score \n",
        "R2=r2_score(y_train,y_pred_train)\n",
        "print(R2)"
      ],
      "metadata": {
        "id": "3JkN0YRZS28e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionary of decision tree results(Train dataset)\n",
        "train_dict={\n",
        "  'Model':\"Decision Tree Regressor-GridSearchCV\",\n",
        "  'MAE':round(MAE,4),\n",
        "  'MSE':round(MAE,4),\n",
        "  'RMSE':round(MAE,4),\n",
        "  'R2_score':round(MAE,4)\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "N_pR4MCnTlVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the figure\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predict\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3yzyOL80YcHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadcity\n",
        "plt.scatter(y_pred,y_test)"
      ],
      "metadata": {
        "id": "RdyLwhvdYxUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest**"
      ],
      "metadata": {
        "id": "htBuZQr5aK4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the packages \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "qM7SFhtWaEdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions on train and test data\n",
        "y_pred_train_r = rf_model.predict(X_train)\n",
        "y_pred_test_r = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "bo8kl6aQbSvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
        "\n",
        "# claculate MSE\n",
        "MSE_rf=mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "# Calculate RMSE \n",
        "RMSE_rf = np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\" ,RMSE_rf)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_rf = mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE ;\" ,MAE_rf)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_rf = r2_score(y_train ,y_pred_train_r)\n",
        "print(\"R2:\",r2_rf)\n",
        "Adjusted_R2_rf = (1-(1-r2_score(y_train ,y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train,y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tcmGJ9CwcR2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((MSE_rf),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_rf),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "Qv7pJTd4eyxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# claculate MSE\n",
        "MSE_rf=mean_squared_error(y_test, y_pred_test_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "# Calculate RMSE \n",
        "RMSE_rf = np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\" ,RMSE_rf)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_rf = mean_absolute_error(y_test, y_pred_test_r)\n",
        "print(\"MAE ;\" ,MAE_rf)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_rf = r2_score(y_test ,y_pred_test_r)\n",
        "print(\"R2:\",r2_rf)\n",
        "Adjusted_R2_rf = (1-(1-r2_score((y_test),(y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test),(y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1) ) )"
      ],
      "metadata": {
        "id": "eIjfHosSDVmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_ score of the test is 0.91. This means that our linear model performs well on the data. Let's try to visualize our residuals to see if there is heteroscedasticity (equal or not different).**"
      ],
      "metadata": {
        "id": "i6vKcj9_FD1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((MSE_rf),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_rf),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "bvJPbGU-FHeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadacity \n",
        "plt.scatter((y_pred_test_r),(y_test-(y_pred_test_r)))"
      ],
      "metadata": {
        "id": "TzGza_xYFYRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "eobUeShGGA-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "\n",
        "importance_dict = {'Feature':list(X_train.columns),\n",
        "                    'Feature Importance': importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n"
      ],
      "metadata": {
        "id": "9LvXBQSgGHpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "0aNAfYcFHUTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "_thLyi-RHnat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit The Model\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "lZgFge4dH8at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "qwMOCafrIL51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Figure\n",
        "plt.figure(figsize=(18,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='green', align='edge')\n",
        "plt.yticks(range(len(indices)), [features[i]for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wq1vWi42IgVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Grdient Boosting**"
      ],
      "metadata": {
        "id": "UX_iyCUyKWIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the packages \n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "MGWwlZXVKfrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions on train and test data\n",
        "y_pred_train_g = gb_model.predict(X_train)\n",
        "y_pred_test_g = gb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "s7Ahr1W8K94G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
        "\n",
        "# claculate MSE\n",
        "MSE_gb=mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "# Calculate RMSE \n",
        "RMSE_gb = np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\" ,RMSE_gb)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_gb = mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE ;\" ,MAE_gb)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_gb = r2_score(y_train ,y_pred_train_g)\n",
        "print(\"R2:\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train,y_pred_train_g)) *((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjuested R2 :\",1-(1-r2_score(y_train,y_pred_train_g)) *((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "ANwmX4gLLLNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It looks like our r2 score is 0.87, which means our model was able to capture most of the data variables. Let's save this in the comparison file for later comparison**"
      ],
      "metadata": {
        "id": "LNE0nhqcSIFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((MSE_gb),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_gb),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "LTxY-nd9ScE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# claculate MSE\n",
        "MSE_gb=mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "# Calculate RMSE \n",
        "RMSE_gb = np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\" ,RMSE_gb)\n",
        "\n",
        "# Calculate MAE\n",
        "MAE_gb = mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE ;\" ,MAE_gb)\n",
        "\n",
        "#import the package\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#Calculate r2 and adjusted r2\n",
        "r2_gb = r2_score(y_test ,y_pred_test_g)\n",
        "print(\"R2:\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test),(y_pred_test_g))) *((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjuested R2 :\",1-(1-r2_score((y_test),(y_pred_test_g))) *((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "ZHgfQGrQTA1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The test's r2_score is 0.86. This means that our linear model performs well on the data. Let's try to show our residuals to see if there is heterosedasticity (even or not scattered).**"
      ],
      "metadata": {
        "id": "Dk1hxoczUPjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadacity \n",
        "plt.scatter((y_pred_test_g),(y_test-(y_pred_test_g)))"
      ],
      "metadata": {
        "id": "yfnnF6qLUdB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "Jocv0pjgUfm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "\n",
        "importance_dict = {'Feature':list(X_train.columns),\n",
        "                    'Feature Importance': importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "6tdp9tVWUwRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "iyYThxj2U-py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "IwNF7UnCVR_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit The Model\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "iYXGOvQbVRmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "GpNkgyT7VROe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Figure\n",
        "plt.figure(figsize=(18,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='green', align='edge')\n",
        "plt.yticks(range(len(indices)), [features[i]for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fMLBXv2IV94v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Hyperparameter tuning** "
      ],
      "metadata": {
        "id": "MGmUNVEta_QB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before we move on to the next model, let's try fine-tuning some hyperparameters and see how our model's performance improves.**\n",
        "\n",
        "**Hyperparameter tuning is the process of selecting the most appropriate hyperparameter set for a learning algorithm. Hyperparameters are parameter values ​​whose values ​​are set before learning starts. The key to machine learning algorithms is hyperparameter tuning.**"
      ],
      "metadata": {
        "id": "zMNLT5_PbVLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using GridSearh**"
      ],
      "metadata": {
        "id": "DqkqKeAMbdOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV helps to pre-evaluate parameters and fit the training model. Finally, we can select the best parameters from the list of hyperparameters.**"
      ],
      "metadata": {
        "id": "3VOOWkDqbtGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of tress\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "\n",
        "# Minimum number of samples required to split a node \n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "#Minimum Number of samples required to split a node \n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# Hyperparameter Grid\n",
        "param_dict = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'max_depth': max_depth,\n",
        "    'min_samples_split' : min_samples_split,\n",
        "    'min_samples_leaf': min_samples_leaf\n",
        "}"
      ],
      "metadata": {
        "id": "8O_hX7j_bxaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "low5kxd8d9w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing Gradient Boosting Regressor** "
      ],
      "metadata": {
        "id": "rxlGqoQOeHGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid Search\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose =2\n",
        "                       )\n",
        "gb_grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "4LUCAxIjeWcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "TVVDQ5aMfbJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "PvZuW5F8kBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_params_"
      ],
      "metadata": {
        "id": "OoRdwBpofsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making predictions on train and test data\n",
        "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_test_g_g = gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Fea_uHTxjg5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score :\",gb_optimal_model.score(X_train,y_train))\n",
        "\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score((y_test) ,(y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n"
      ],
      "metadata": {
        "id": "iBcKMdO_iCbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((MSE_gbh),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_gbh),2)\n",
        "}\n",
        "\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "Rf4OZ8d6i-Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Package\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "MSE_gbh= mean_squared_error(y_test, y_pred_test_g_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_test_g_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score((y_test) ,(y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "-rQfa6YwlqbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((MSE_gbh),3),\n",
        "       'Adjuested R2':round((Adjusted_R2_gbh),2)\n",
        "}\n",
        "\n",
        "test_df = pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "a78ReGCqmRTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hetroscadacity \n",
        "plt.scatter((y_pred_test_g_g),(y_test-(y_pred_test_g_g)))"
      ],
      "metadata": {
        "id": "hPhoeocloB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "qsF8ZVKaoL02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_optimal_model.feature_importances_\n",
        "importance_dict = {'Feature':list(X_train.columns),\n",
        "                    'Feature Importance': importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "SDZAYWXWoZCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "2pnVqOVMo6_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "Iy9cufO6pF3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "q6-6iN6xpHIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit The Model\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "ctX0HdUxpW9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "Yq1j0UMopZIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Figure\n",
        "plt.figure(figsize=(18,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='green', align='edge')\n",
        "plt.yticks(range(len(indices)), [features[i]for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OQZv89i4pyxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "o5jKMXIsp6u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**During the analysis, we started to do EDA on all the features of the data set. We identified and changed the first three aspects of \"Rented Bike Count\". Next, we analyzed categorical variables by removing the most common variables in a category, and also analyzed numerical variables to see their correlations, distributions, and effects corresponding to the difference between the two. We've also removed some of the most common 0-valued and single-code categorical variables.**\n",
        "\n",
        "**Next we use 7 machine learning algorithms Linear Regression, lasso, ridge, elastic mesh, cut trees, Random Forest and XGBoost.\n",
        "We made hyperparameter tuning to improve the performance of our model. The results of our evaluation:**\n",
        "\n",
        "**No optimum performance.**\n",
        "\n",
        "**Random forest regressor and gradient assisted gridsearchcv give the highest R2 scores of 99% and 95% on the training set and 92% on the test set.**\n",
        "\n",
        "**Random forest and gradient boost have significant property differences.**\n",
        "\n",
        "**We can deploy this model.**\n",
        "\n",
        "**However, this is not the final result. Because these data are time dependent, variables such as temperature, wind speed and solar radiation are not necessarily important. Therefore, there are situations when the model does not perform well. Since machine learning is a constantly evolving process, we need to be prepared for any eventuality and check our models frequently.\n",
        "Therefore, having a good knowledge and following the development of machine learning will help the person in the future.**"
      ],
      "metadata": {
        "id": "aozt0fQqqSa9"
      }
    }
  ]
}